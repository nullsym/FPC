#!/usr/bin/env python

import requests
import re, datetime
import sqlite3
from lxml import html
import argparse

# Import FPL code
import sys, os
sys.path.append(os.path.join(os.path.dirname(__file__), '../', 'lib'))
import fpc

###############
# Global vars #
###############
HOURS = list(range(0, 24))
TOTAL_MONEY = 0
TOTAL_KWH = 0
MONEY = None
KWH = None
CENTS_KWH = []
TEMPERATURE = None
DATE = None
COMMENTS = None
# Misc
DB_NAME = fpc.DIR_NAME + "/web/data.db"
DB = None

#############
# Crawl FPL #
#############
def crawl_website(dry_run, days_range):
    # Create the session object
    session_requests = requests.session()
    # Fetch needed cookies
    result = fpc.get_session("daily")
    # Fetch needed data for our payload
    find_match = re.compile('var premiseNumber = (.*);')
    premise_number = find_match.findall(result.text)[0][1:-1]
    find_match = re.compile('var certifiedDate = (.*);')
    certified_date = find_match.findall(result.text)[0][1:-1]
    certified_date = datetime.datetime.strptime(certified_date, "%b %d, %Y")
    certified_date = certified_date.strftime("%Y/%m/%d")
    url = "https://app.fpl.com/wps/PA_ESFPortalWeb/getHourlyConsumption.do"

    # Start with 1 because the server won't have information for the current day (0)
    for i in range(days_range[0], days_range[1]):
        start_date = datetime.date.today() - datetime.timedelta(i)
        start_date = str(start_date) + "T00:00:00"

        payload = {'tempType': 'max',
                'ecmonthHumType': 'NoHum',
                'viewType': 'dollar',
                'userType': 'EXT',
                'ecHasMoveInRate': 'false',
                'isResidential': 'true',
                'isTouUser': 'false',
                'showGroupData': 'false',
                'isNetMeter': 'false',
                'ecShowMinTab': 'true',
                'isMultiMeter': 'false',
                'accountType': 'ELE',
                'zipCode': fpc.settings("zip"),
                'accountNumber': fpc.settings("account_number"),
                'premiseNumber': premise_number,
                'certifiedDate': certified_date,
                'startDate' : start_date,
                'endDate': start_date
                }
        result = session_requests.post(url, data=payload)

        # data_fetch() populates the global variables:
        # MONEY, KWH, TEMPERATURE, DATE,
        # TOTAL_MONEY, and TOTAL_KWH.
        if data_fetch(result.content, result.text):
            if dry_run:
                data_print()
            else:
                db_insert()
        else:
            print("Could not retrieve info")
    # End of for loop

# Get the data grabbed from the crawling step and
# make it ready for insertion into the database
def data_fetch(xml_byte, xml_text):
    global MONEY, KWH, TEMPERATURE, DATE
    global TOTAL_MONEY, TOTAL_KWH
    tree = html.fromstring(xml_byte)

    # The XML has 4 datasets: Money, Temp, kWh, Temp.
    # In that order. The Temp dataset is repeated.
    datasets = tree.xpath("//dataset")
    try:
        MONEY = datasets[0]
        TEMPERATURE = datasets[1]
        KWH = datasets[2]
    except IndexError:
        return(False)

    # FPL returns an empty string as the value of KWH if we haven't used
    # electricity. It charges a fixed price for those hours.
    # We need to specify that the KWH is zero.
    for i in range(0, 24):
        # What happens when you cut your electricity at X hour?
        # We need to try this for every hour.
        if KWH[i].attrib['value'] == "":
            KWH[i].attrib['value'] = "0"

    # Perform a non-greedy search since the string is repeated
    # two times throughout the document
    find_match = re.compile('Usage for: </b>(.*?). &')
    DATE = find_match.search(xml_text)
    DATE = DATE.group(1)

    cent_per_kwh = 0
    del CENTS_KWH[:]
    # Get the total money and total kwh from that day
    # As well as the cents per kWh for each hour
    for i in range(0, 24):
        cents_per_kwh = round(float(MONEY[i].attrib['value']) / float(KWH[i].attrib['value']), 2) # Division by zero?
        CENTS_KWH.append(cents_per_kwh)
        TOTAL_KWH += float(KWH[i].attrib['value'])
        TOTAL_MONEY += float(MONEY[i].attrib['value'])
    TOTAL_KWH = round(TOTAL_KWH, 3)
    TOTAL_MONEY = round(TOTAL_MONEY, 2)

    return(True)


######################
# Printing functions #
######################
def data_print():
    print(DATE)
    # Sometimes the XML does not have temperature information
    if bool(TEMPERATURE[0].attrib):
        print("+-----------------------------------+")
        print("| Hour  | Money   | kWh   | Temp    |")
        print("+-----------------------------------+")
        for i in range(0, 24):
            print("| {:5} | ${:6} | {:6} | {:6} |".format(HOURS[i],
                MONEY[i].attrib['value'],
                KWH[i].attrib['value'],
                TEMPERATURE[i].attrib['value']))
        print("+-----------------------------------+")

    else:
        sys.exit("The XML file didn't give us the needed temperatures")

def db_print():
    print("+-----------------------------------------------------+")
    print("| Date      | Hour   | Money  | kWh    | kWhc |  Temp |")
    print("+-----------------------------------------------------+")
    for row in DB.execute("SELECT * FROM FPC ORDER BY date DESC limit 14"):
        print("| {:5} | {:6} | {:6} | {:6} | {:5} | {:4} |".format(row[0],
            row[1], row[2], row[3], row[4], row[5]))
        # print(row)

    print("+------------------------------------------+")
    print("| Date      | Date       | Money   | kWh   |")
    print("+------------------------------------------+")
    for row in DB.execute("SELECT * FROM Summary ORDER BY date DESC limit 21"):
        print("| {:5} | {:6} | {:6} | {:6} |".format(row[0],
            row[1], row[2], row[3]))
        # print(row)

################
# DB Functions #
################
def db_insert():
    global TOTAL_MONEY, TOTAL_KWH
    # FPL uses at least these two formats for dates:
    # Tuesday May  31, 2016
    # Wednesday Jun. 01, 2016
    if "." in DATE:
        date = datetime.datetime.strptime(DATE, "%A %b. %d, %Y")
    else:
        date = datetime.datetime.strptime(DATE, "%A %b %d, %Y")

    date_db = date.strftime("%Y-%W-%u") # Eg 2016-32-1

    # Do not INSERT data for a day that already has entries in the DB
    DB.execute("SELECT rowid FROM FPC WHERE date = (?)", (date_db,))
    if DB.fetchone() is None:
        print("Inserting '" + DATE + "' into the DB")

        # A tuple is expected by executemany. Create and populate it.
        my_data = []
        for i in range(0, 24):
            my_data.append((date_db,
                HOURS[i],
                MONEY[i].attrib['value'],
                KWH[i].attrib['value'],
                CENTS_KWH[i],
                TEMPERATURE[i].attrib['value']))

        # Insert populated tuple into table
        DB.executemany('INSERT INTO FPC VALUES (?, ?, ?, ?, ?, ?)', my_data)

        # Insert the tuple into table SUMMARY
        date_summary = date.strftime("%a %b %d") # Eg Mon Aug 22
        my_data2 = [(date_db, date_summary, TOTAL_MONEY, TOTAL_KWH, COMMENTS)]
        DB.executemany('INSERT INTO SUMMARY VALUES (?, ?, ?, ?, ?)', my_data2)

    else:
        print("The information for this date is already in the database")

    # Clear the vars
    TOTAL_MONEY = 0
    TOTAL_KWH = 0

def db_delete(dry_run, days_range):
    print('TODO: Deleting from...' + str(days_range[0]) + ' to...' + str(days_range[1]))

########
# Main #
########
if __name__ == '__main__':
    if not (os.path.isfile(DB_NAME)):
        # +--------------------------------------------------+
        # |                  Table FPC                       |
        # |--------------------------------------------------|
        # | Date      | Hour | $  | kWh | kWh rate  | Temp   |
        # |--------------------------------------------------|
        # | 2016-29-2 | 2    | 34 | 2   |  0.39     | 80     |
        # +--------------------------------------------------+
        # +------------------------------------------------+
        # |                 Table SUMMARY                  |
        # +------------------------------------------------+
        # | Date      | dateprnt   | $    | kWh  | Comments|
        # |------------------------------------------------|
        # | 2016-34-1 | Mon Aug 22 | 2.01 | 13   | None    |
        # +------------------------------------------------+
        print("[INFO] Creating the database")
        conn = sqlite3.connect(DB_NAME)
        DB = conn.cursor()

        DB.execute('''CREATE TABLE FPC
                (date text, hours tinyint, money real, kwh real, money_kwh real, temp tinyint)''')

        DB.execute('''CREATE TABLE SUMMARY
                (date text, dateprnt text, money real, kwh real, comments text)''')

        conn.commit()
        conn.close()

    # Open the DB
    conn = sqlite3.connect(DB_NAME)
    DB = conn.cursor()

    parser = argparse.ArgumentParser(description="Populate the database with our daily electricity usage")
    parser.add_argument("--dry-run", action="store_true",
            help="Do not commit the changes to the databse")
    parser.add_argument("-d", "--delete", action="store_true",
            help="Delete the specified range from the database")
    parser.add_argument("-p", "--print", action="store_true",
            help="Print the data in the database")
    parser.add_argument("-r", "--range", metavar='N', type=int, nargs=2,
            default=[1, 2], help="Range of days to fetch. Defaults to '1 2'.")
    args = parser.parse_args()

    # Sanity check: r[0] HAS to be less than r[1]
    if (args.range[0] >= args.range[1]):
        sys.exit("The first arg to the range has to be less than the second arg")
    # Print the latest entries of the database
    elif args.print:
        db_print()
    # Delete a range from the database
    elif args.delete:
        db_delete(args.dry_run, args.range)
    # Display the crawled data without saving it
    elif args.dry_run:
        print("Performing a dry run, there will be NO changes to the DB.")
        crawl_website(args.dry_run, args.range)
    # Crawl and save
    else:
        crawl_website(args.dry_run, args.range)

    # Close the DB
    conn.commit()
    conn.close()
